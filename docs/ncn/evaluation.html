<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.2" />
<title>ncn.evaluation API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ncn.evaluation</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import logging
import json
import pickle
from operator import itemgetter
import warnings
from typing import Dict, Tuple, List, Union

import torch
from torch import nn
import torch.nn.functional as F
from gensim.summarization.bm25 import BM25
from torchtext.data import TabularDataset
from tqdm import tqdm_notebook

import ncn.core
from ncn.core import BaseData, Stringlike, PathOrStr, DEVICE, Filters
from ncn.model import NeuralCitationNetwork

logger = logging.getLogger(&#34;neural_citation.evaluation&#34;)


# TODO: Document this
class Evaluator:
    &#34;&#34;&#34;
    Evaluator class for the neural citation network. Uses a trained NCN model and BM-25 to perform
    evaluation tasks on the test set or inference on the full dataset. 
    
    ## Parameters:  
    
    - **path_to_weights** *(PathOrStr)*: Path to the weights of a pretrained NCN model. 
    - **data** *(BaseData)*: BaseData container holding train, valid, and test data.
        Also holds initialized context, title and author fields.  
    - **evaluate** *(bool=True)*: Determines the size of the BM-25 corpus used.
        If True, only the test samples will be used (model evaluation mode).
        If False, the corpus is built from the complete dataset (inference mode).   
    &#34;&#34;&#34;
    def __init__(self, context_filters: Filters, author_filters: Filters,
                 num_filters: int, embed_size:int,
                 path_to_weights: PathOrStr, data: BaseData, 
                 evaluate: bool = True, show_attention: bool = False):
        self.data = data
        self.context, self.title, self.authors = self.data.cntxt, self.data.ttl, self.data.aut
        self.pad = self.title.vocab.stoi[&#39;&lt;pad&gt;&#39;]
        self.criterion = nn.CrossEntropyLoss(ignore_index = self.pad, reduction=&#34;none&#34;)

        self.model = NeuralCitationNetwork(context_filters=context_filters,
                                            author_filters=author_filters,
                                            context_vocab_size=len(self.context.vocab),
                                            title_vocab_size=len(self.title.vocab),
                                            author_vocab_size=len(self.authors.vocab),
                                            pad_idx=self.pad,
                                            num_filters=num_filters,
                                            authors=True, 
                                            embed_size=embed_size,
                                            num_layers=2,
                                            hidden_size=num_filters,
                                            dropout_p=0.2,
                                            show_attention=True)
        self.model.to(DEVICE)
        self.model.load_state_dict(torch.load(path_to_weights, map_location=DEVICE))
        self.model.eval()
        logger.info(self.model.settings)

        self.eval = evaluate
        self.show_attention = show_attention

        # instantiate examples, corpus and bm25 depending on mode
        logger.info(f&#34;Creating corpus in eval={self.eval} mode.&#34;)
        if self.eval:
            self.examples = data.test.examples
            logger.info(f&#34;Number of samples in BM25 corpus: {len(self.examples)}&#34;)
            self.corpus = list(set([tuple(example.title_cited) for example in self.examples]))
            self.bm25 = BM25(self.corpus)
            self.context_cited_indices = self._get_context_title_indices(self.examples)
        else:
            self.examples = data.train.examples + data.train.examples+ data.train.examples
            logger.info(f&#34;Number of samples in BM25 corpus: {len(self.examples)}&#34;)
            self.corpus = list(set([tuple(example.title_cited) for example in self.examples]))
            self.bm25 = BM25(self.corpus)
            
            # load mapping to give proper recommendations
            with open(&#34;assets/title_tokenized_to_full.pkl&#34;, &#34;rb&#34;) as fp:
                self.title_to_full = pickle.load(fp)

        
        with open(&#34;assets/title_to_aut_cited.pkl&#34;, &#34;rb&#34;) as fp:
            self.title_aut_cited = pickle.load(fp)

    @staticmethod
    def _get_context_title_indices(examples: List) -&gt; Dict[Tuple[str, ...], List[str]]:
        mapping = {}
        for i, example in enumerate(examples):
            key = tuple(example.context)
            if key not in mapping.keys():
                mapping[key] = [i]
            else:
                mapping[key].append(i)
        
        return mapping

    def _get_bm_top(self, query: List[str]) -&gt; List[List[str]]:
        &#34;&#34;&#34;
        Uses BM-25 to compute the most similar titles in the corpus given a query. 
        The query can either be passed as string or a list of strings (tokenized string). 
        Returns the tokenized most similar corpus titles.
        Only titles with similarity values &gt; 0 are returned.
        A maximum number of 2048 titles is returned in eval mode. 
        For recommendations, the top 256 titles are returned.  

        ## Parameters:  
    
        - **query** *(Stringlike)*: Query in string or tokenized form. 

        ## Output:  
        
        - **indices** *(List[int])*: List of corpus indices with the highest similary rating to the query.   
        &#34;&#34;&#34;
        # sort titles according to score and return indices
        scores = [(score, title) for score, title in zip(self.bm25.get_scores(query), self.corpus)]
        scores = sorted(scores, key=itemgetter(0), reverse=True)

        # Return top 2048 for evaluation purpose, cut to half for recommendations to prevent memory errors
        if self.eval:
            try:
                return [title for score, title in scores][:256]
            except IndexError:
                return [title for score, title in scores]
        else:
            try:
                return [title for score, title in scores if score &gt; 0][:1028]
            except IndexError:
                return [title for score, title in scores if score &gt; 0]


    def recall(self, x: int) -&gt; Union[float, List[float]]:
        &#34;&#34;&#34;
        Computes recall @x metric on the test set for model evaluation purposes.  
        
        ## Parameters:  
        * *(shapes)
        - **x** *(int)*: Specifies at which level the recall is computed.  
        
        ## Output:  
        
        - **recall** *(Union[float, List[float]])*: Float or list of floats with recall @x value.    
        &#34;&#34;&#34;
        if not self.eval: warnings.warn(&#34;Performing evaluation on all data. This hurts performance.&#34;, RuntimeWarning)
        
        recall_list = []
        with torch.no_grad():
            for example in tqdm_notebook(self.data.test[:20000]):
                # numericalize query
                context = self.context.numericalize([example.context])
                citing = self.context.numericalize([example.authors_citing])
                context = context.to(DEVICE)
                citing = citing.to(DEVICE)

                # catch contexts and citings shorter than filter lengths and pad manually
                if context.shape[1] &lt; 5:
                    assert self.pad == 1, &#34;Padding index doesn&#39;t match tensor index!&#34;
                    padded = torch.ones((1,5), dtype=torch.long)
                    padded[:, :context.shape[1]] = context
                    context = padded.to(DEVICE)
                if citing.shape[1] &lt; 2:
                    padded = torch.ones((1,2), dtype=torch.long)
                    padded[:, :citing.shape[1]] = citing
                    citing = padded.to(DEVICE)

                top_titles = self._get_bm_top(example.context)
                logger.info(f&#34;True title in top_titles: {example.title_cited in top_titles}.&#34;)
                top_authors = [self.title_aut_cited[tuple(title)] for title in top_titles]
                
                # add all true cited titles (can be multiple per context)
                indices = self.context_cited_indices[tuple(example.context)]
                append_count = 0
                for i in indices: 
                    top_titles.append(self.examples[i].title_cited)
                    top_authors.append(self.examples[i].authors_cited)
                    append_count += 1

                

                logger.debug(f&#34;Number of candidate authors {len(top_authors)}.&#34;)
                logger.debug(f&#34;Number of candidate titles {len(top_titles)}.&#34;)
                assert len(top_authors) == len(top_titles), &#34;Evaluation title and author lengths don&#39;t match!&#34;

                # prepare batches
                citeds = self.authors.numericalize(self.authors.pad(top_authors))
                titles = self.title.numericalize(self.title.pad(top_titles))
                citeds = citeds.to(DEVICE)
                titles = titles.to(DEVICE)

                # repeat context and citing to len(indices) and calculate loss for single, large batch
                context = context.repeat(len(top_titles), 1)
                citing = citing.repeat(len(top_titles), 1)
                msg = &#34;Evaluation batch sizes don&#39;t match!&#34;
                assert context.shape[0] == citing.shape[0] == citeds.shape[0] == titles.shape[1], msg

                logger.debug(f&#34;Context shape: {context.shape}.&#34;)
                logger.debug(f&#34;Citing shape: {citing.shape}.&#34;)
                logger.debug(f&#34;Titles shape: {titles.shape}.&#34;)
                logger.debug(f&#34;Citeds shape: {citeds.shape}.&#34;)

                # calculate scores
                output = self.model(context = context, title = titles, authors_citing = citing, authors_cited = citeds)
                output = output[1:].permute(1,2,0)
                titles = titles[1:].permute(1,0)

                logger.debug(f&#34;Evaluation output shapes: {output.shape}&#34;)
                logger.debug(f&#34;Evaluation title shapes: {titles.shape}&#34;)

                scores = self.criterion(output, titles)
                scores = scores.sum(dim=1)
                logger.info(f&#34;Evaluation scores shape: {scores.shape}&#34;)

                _, index = scores.topk(x, largest=False, sorted=True, dim=0)

                logger.info(f&#34;Index: {index}&#34;)
                logger.info(f&#34;Lowest scores: {scores[index]}&#34;)
                logger.info(f&#34;Range of true titles: {len(top_titles) - 1} - {len(top_titles) - 1 - append_count}&#34;)

                # check how many of the concatenated (=true) titles have been returned
                scored = 0
                for i in range(append_count):
                    if len(top_titles) - (i + 1) in index: scored += 1
                    
                logger.info(f&#34;Scored {scored} out of {append_count} titles at {x}.&#34;)
                
                recall_list.append(scored/append_count)

            return sum(recall_list) / len(self.data.test[:20000])
        
    def recommend(self, query: Stringlike, citing: Stringlike, top_x: int = 5):
        if self.eval: warnings.warn(&#34;Performing inference only on the test set.&#34;, RuntimeWarning)
        
        if isinstance(query, str): 
            query = self.context.tokenize(query)
        if isinstance(citing, str):
            citing = self.authors.tokenize(citing)
         
        with torch.no_grad():
            top_titles = self._get_bm_top(query)
            top_authors = [self.title_aut_cited[tuple(title)] for title in top_titles]
            assert len(top_authors) == len(top_titles), &#34;Evaluation title and author lengths don&#39;t match!&#34;

            context = self.context.numericalize([query])
            citing = self.context.numericalize([citing])
            context = context.to(DEVICE)
            citing = citing.to(DEVICE)

            # prepare batches
            citeds = self.authors.numericalize(self.authors.pad(top_authors))
            titles = self.title.numericalize(self.title.pad(top_titles))
            citeds = citeds.to(DEVICE)
            titles = titles.to(DEVICE)

            logger.debug(f&#34;Evaluation title shapes: {titles.shape}&#34;)

            # repeat context and citing to len(indices) and calculate loss for single, large batch
            context = context.repeat(len(top_titles), 1)
            citing = citing.repeat(len(top_titles), 1)
            msg = &#34;Evaluation batch sizes don&#39;t match!&#34;
            assert context.shape[0] == citing.shape[0] == citeds.shape[0] == titles.shape[1], msg

            # calculate scores
            if self.show_attention:
                output, attention = self.model(context = context, title = titles, 
                                               authors_citing = citing, authors_cited = citeds)
            else:
                output = self.model(context = context, title = titles, 
                                    authors_citing = citing, authors_cited = citeds)
            output = output[1:].permute(1,2,0)
            titles = titles[1:].permute(1,0)

            logger.debug(f&#34;Evaluation output shapes: {output.shape}&#34;)
            logger.debug(f&#34;Evaluation title shapes: {titles.shape}&#34;)

            scores = self.criterion(output, titles)
            scores = scores.sum(dim=1)
            logger.debug(f&#34;Evaluation scores shape: {scores.shape}&#34;)
            _, index = scores.topk(top_x, largest=False, sorted=True, dim=0)

            recommended = [&#34; &#34;.join(top_titles[i]) for i in index]
        
        if self.show_attention:
            return {i: self.title_to_full[title] for i, title in enumerate(recommended)}, attention[:, index, :]

        return {i: self.title_to_full[title] for i, title in enumerate(recommended)}

        </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ncn.evaluation.Evaluator"><code class="flex name class">
<span>class <span class="ident">Evaluator</span></span>
<span>(</span><span>context_filters: List[int], author_filters: List[int], num_filters: int, embed_size: int, path_to_weights: Union[pathlib.Path, str], data: <a title="ncn.core.BaseData" href="core.html#ncn.core.BaseData">BaseData</a>, evaluate: bool = True, show_attention: bool = False)</span>
</code></dt>
<dd>
<section class="desc"><p>Evaluator class for the neural citation network. Uses a trained NCN model and BM-25 to perform
evaluation tasks on the test set or inference on the full dataset. </p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li><strong>path_to_weights</strong> <em>(PathOrStr)</em>: Path to the weights of a pretrained NCN model. </li>
<li><strong>data</strong> <em>(BaseData)</em>: BaseData container holding train, valid, and test data.
Also holds initialized context, title and author fields.
</li>
<li><strong>evaluate</strong> <em>(bool=True)</em>: Determines the size of the BM-25 corpus used.
If True, only the test samples will be used (model evaluation mode).
If False, the corpus is built from the complete dataset (inference mode).</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Evaluator:
    &#34;&#34;&#34;
    Evaluator class for the neural citation network. Uses a trained NCN model and BM-25 to perform
    evaluation tasks on the test set or inference on the full dataset. 
    
    ## Parameters:  
    
    - **path_to_weights** *(PathOrStr)*: Path to the weights of a pretrained NCN model. 
    - **data** *(BaseData)*: BaseData container holding train, valid, and test data.
        Also holds initialized context, title and author fields.  
    - **evaluate** *(bool=True)*: Determines the size of the BM-25 corpus used.
        If True, only the test samples will be used (model evaluation mode).
        If False, the corpus is built from the complete dataset (inference mode).   
    &#34;&#34;&#34;
    def __init__(self, context_filters: Filters, author_filters: Filters,
                 num_filters: int, embed_size:int,
                 path_to_weights: PathOrStr, data: BaseData, 
                 evaluate: bool = True, show_attention: bool = False):
        self.data = data
        self.context, self.title, self.authors = self.data.cntxt, self.data.ttl, self.data.aut
        self.pad = self.title.vocab.stoi[&#39;&lt;pad&gt;&#39;]
        self.criterion = nn.CrossEntropyLoss(ignore_index = self.pad, reduction=&#34;none&#34;)

        self.model = NeuralCitationNetwork(context_filters=context_filters,
                                            author_filters=author_filters,
                                            context_vocab_size=len(self.context.vocab),
                                            title_vocab_size=len(self.title.vocab),
                                            author_vocab_size=len(self.authors.vocab),
                                            pad_idx=self.pad,
                                            num_filters=num_filters,
                                            authors=True, 
                                            embed_size=embed_size,
                                            num_layers=2,
                                            hidden_size=num_filters,
                                            dropout_p=0.2,
                                            show_attention=True)
        self.model.to(DEVICE)
        self.model.load_state_dict(torch.load(path_to_weights, map_location=DEVICE))
        self.model.eval()
        logger.info(self.model.settings)

        self.eval = evaluate
        self.show_attention = show_attention

        # instantiate examples, corpus and bm25 depending on mode
        logger.info(f&#34;Creating corpus in eval={self.eval} mode.&#34;)
        if self.eval:
            self.examples = data.test.examples
            logger.info(f&#34;Number of samples in BM25 corpus: {len(self.examples)}&#34;)
            self.corpus = list(set([tuple(example.title_cited) for example in self.examples]))
            self.bm25 = BM25(self.corpus)
            self.context_cited_indices = self._get_context_title_indices(self.examples)
        else:
            self.examples = data.train.examples + data.train.examples+ data.train.examples
            logger.info(f&#34;Number of samples in BM25 corpus: {len(self.examples)}&#34;)
            self.corpus = list(set([tuple(example.title_cited) for example in self.examples]))
            self.bm25 = BM25(self.corpus)
            
            # load mapping to give proper recommendations
            with open(&#34;assets/title_tokenized_to_full.pkl&#34;, &#34;rb&#34;) as fp:
                self.title_to_full = pickle.load(fp)

        
        with open(&#34;assets/title_to_aut_cited.pkl&#34;, &#34;rb&#34;) as fp:
            self.title_aut_cited = pickle.load(fp)

    @staticmethod
    def _get_context_title_indices(examples: List) -&gt; Dict[Tuple[str, ...], List[str]]:
        mapping = {}
        for i, example in enumerate(examples):
            key = tuple(example.context)
            if key not in mapping.keys():
                mapping[key] = [i]
            else:
                mapping[key].append(i)
        
        return mapping

    def _get_bm_top(self, query: List[str]) -&gt; List[List[str]]:
        &#34;&#34;&#34;
        Uses BM-25 to compute the most similar titles in the corpus given a query. 
        The query can either be passed as string or a list of strings (tokenized string). 
        Returns the tokenized most similar corpus titles.
        Only titles with similarity values &gt; 0 are returned.
        A maximum number of 2048 titles is returned in eval mode. 
        For recommendations, the top 256 titles are returned.  

        ## Parameters:  
    
        - **query** *(Stringlike)*: Query in string or tokenized form. 

        ## Output:  
        
        - **indices** *(List[int])*: List of corpus indices with the highest similary rating to the query.   
        &#34;&#34;&#34;
        # sort titles according to score and return indices
        scores = [(score, title) for score, title in zip(self.bm25.get_scores(query), self.corpus)]
        scores = sorted(scores, key=itemgetter(0), reverse=True)

        # Return top 2048 for evaluation purpose, cut to half for recommendations to prevent memory errors
        if self.eval:
            try:
                return [title for score, title in scores][:256]
            except IndexError:
                return [title for score, title in scores]
        else:
            try:
                return [title for score, title in scores if score &gt; 0][:1028]
            except IndexError:
                return [title for score, title in scores if score &gt; 0]


    def recall(self, x: int) -&gt; Union[float, List[float]]:
        &#34;&#34;&#34;
        Computes recall @x metric on the test set for model evaluation purposes.  
        
        ## Parameters:  
        * *(shapes)
        - **x** *(int)*: Specifies at which level the recall is computed.  
        
        ## Output:  
        
        - **recall** *(Union[float, List[float]])*: Float or list of floats with recall @x value.    
        &#34;&#34;&#34;
        if not self.eval: warnings.warn(&#34;Performing evaluation on all data. This hurts performance.&#34;, RuntimeWarning)
        
        recall_list = []
        with torch.no_grad():
            for example in tqdm_notebook(self.data.test[:20000]):
                # numericalize query
                context = self.context.numericalize([example.context])
                citing = self.context.numericalize([example.authors_citing])
                context = context.to(DEVICE)
                citing = citing.to(DEVICE)

                # catch contexts and citings shorter than filter lengths and pad manually
                if context.shape[1] &lt; 5:
                    assert self.pad == 1, &#34;Padding index doesn&#39;t match tensor index!&#34;
                    padded = torch.ones((1,5), dtype=torch.long)
                    padded[:, :context.shape[1]] = context
                    context = padded.to(DEVICE)
                if citing.shape[1] &lt; 2:
                    padded = torch.ones((1,2), dtype=torch.long)
                    padded[:, :citing.shape[1]] = citing
                    citing = padded.to(DEVICE)

                top_titles = self._get_bm_top(example.context)
                logger.info(f&#34;True title in top_titles: {example.title_cited in top_titles}.&#34;)
                top_authors = [self.title_aut_cited[tuple(title)] for title in top_titles]
                
                # add all true cited titles (can be multiple per context)
                indices = self.context_cited_indices[tuple(example.context)]
                append_count = 0
                for i in indices: 
                    top_titles.append(self.examples[i].title_cited)
                    top_authors.append(self.examples[i].authors_cited)
                    append_count += 1

                

                logger.debug(f&#34;Number of candidate authors {len(top_authors)}.&#34;)
                logger.debug(f&#34;Number of candidate titles {len(top_titles)}.&#34;)
                assert len(top_authors) == len(top_titles), &#34;Evaluation title and author lengths don&#39;t match!&#34;

                # prepare batches
                citeds = self.authors.numericalize(self.authors.pad(top_authors))
                titles = self.title.numericalize(self.title.pad(top_titles))
                citeds = citeds.to(DEVICE)
                titles = titles.to(DEVICE)

                # repeat context and citing to len(indices) and calculate loss for single, large batch
                context = context.repeat(len(top_titles), 1)
                citing = citing.repeat(len(top_titles), 1)
                msg = &#34;Evaluation batch sizes don&#39;t match!&#34;
                assert context.shape[0] == citing.shape[0] == citeds.shape[0] == titles.shape[1], msg

                logger.debug(f&#34;Context shape: {context.shape}.&#34;)
                logger.debug(f&#34;Citing shape: {citing.shape}.&#34;)
                logger.debug(f&#34;Titles shape: {titles.shape}.&#34;)
                logger.debug(f&#34;Citeds shape: {citeds.shape}.&#34;)

                # calculate scores
                output = self.model(context = context, title = titles, authors_citing = citing, authors_cited = citeds)
                output = output[1:].permute(1,2,0)
                titles = titles[1:].permute(1,0)

                logger.debug(f&#34;Evaluation output shapes: {output.shape}&#34;)
                logger.debug(f&#34;Evaluation title shapes: {titles.shape}&#34;)

                scores = self.criterion(output, titles)
                scores = scores.sum(dim=1)
                logger.info(f&#34;Evaluation scores shape: {scores.shape}&#34;)

                _, index = scores.topk(x, largest=False, sorted=True, dim=0)

                logger.info(f&#34;Index: {index}&#34;)
                logger.info(f&#34;Lowest scores: {scores[index]}&#34;)
                logger.info(f&#34;Range of true titles: {len(top_titles) - 1} - {len(top_titles) - 1 - append_count}&#34;)

                # check how many of the concatenated (=true) titles have been returned
                scored = 0
                for i in range(append_count):
                    if len(top_titles) - (i + 1) in index: scored += 1
                    
                logger.info(f&#34;Scored {scored} out of {append_count} titles at {x}.&#34;)
                
                recall_list.append(scored/append_count)

            return sum(recall_list) / len(self.data.test[:20000])
        
    def recommend(self, query: Stringlike, citing: Stringlike, top_x: int = 5):
        if self.eval: warnings.warn(&#34;Performing inference only on the test set.&#34;, RuntimeWarning)
        
        if isinstance(query, str): 
            query = self.context.tokenize(query)
        if isinstance(citing, str):
            citing = self.authors.tokenize(citing)
         
        with torch.no_grad():
            top_titles = self._get_bm_top(query)
            top_authors = [self.title_aut_cited[tuple(title)] for title in top_titles]
            assert len(top_authors) == len(top_titles), &#34;Evaluation title and author lengths don&#39;t match!&#34;

            context = self.context.numericalize([query])
            citing = self.context.numericalize([citing])
            context = context.to(DEVICE)
            citing = citing.to(DEVICE)

            # prepare batches
            citeds = self.authors.numericalize(self.authors.pad(top_authors))
            titles = self.title.numericalize(self.title.pad(top_titles))
            citeds = citeds.to(DEVICE)
            titles = titles.to(DEVICE)

            logger.debug(f&#34;Evaluation title shapes: {titles.shape}&#34;)

            # repeat context and citing to len(indices) and calculate loss for single, large batch
            context = context.repeat(len(top_titles), 1)
            citing = citing.repeat(len(top_titles), 1)
            msg = &#34;Evaluation batch sizes don&#39;t match!&#34;
            assert context.shape[0] == citing.shape[0] == citeds.shape[0] == titles.shape[1], msg

            # calculate scores
            if self.show_attention:
                output, attention = self.model(context = context, title = titles, 
                                               authors_citing = citing, authors_cited = citeds)
            else:
                output = self.model(context = context, title = titles, 
                                    authors_citing = citing, authors_cited = citeds)
            output = output[1:].permute(1,2,0)
            titles = titles[1:].permute(1,0)

            logger.debug(f&#34;Evaluation output shapes: {output.shape}&#34;)
            logger.debug(f&#34;Evaluation title shapes: {titles.shape}&#34;)

            scores = self.criterion(output, titles)
            scores = scores.sum(dim=1)
            logger.debug(f&#34;Evaluation scores shape: {scores.shape}&#34;)
            _, index = scores.topk(top_x, largest=False, sorted=True, dim=0)

            recommended = [&#34; &#34;.join(top_titles[i]) for i in index]
        
        if self.show_attention:
            return {i: self.title_to_full[title] for i, title in enumerate(recommended)}, attention[:, index, :]

        return {i: self.title_to_full[title] for i, title in enumerate(recommended)}</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="ncn.evaluation.Evaluator.recall"><code class="name flex">
<span>def <span class="ident">recall</span></span>(<span>self, x: int) -> Union[float, List[float]]</span>
</code></dt>
<dd>
<section class="desc"><p>Computes recall @x metric on the test set for model evaluation purposes.
</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li>*(shapes)</li>
<li><strong>x</strong> <em>(int)</em>: Specifies at which level the recall is computed.
</li>
</ul>
<h2 id="output">Output:</h2>
<ul>
<li><strong>recall</strong> <em>(Union[float, List[float]])</em>: Float or list of floats with recall @x value.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def recall(self, x: int) -&gt; Union[float, List[float]]:
    &#34;&#34;&#34;
    Computes recall @x metric on the test set for model evaluation purposes.  
    
    ## Parameters:  
    * *(shapes)
    - **x** *(int)*: Specifies at which level the recall is computed.  
    
    ## Output:  
    
    - **recall** *(Union[float, List[float]])*: Float or list of floats with recall @x value.    
    &#34;&#34;&#34;
    if not self.eval: warnings.warn(&#34;Performing evaluation on all data. This hurts performance.&#34;, RuntimeWarning)
    
    recall_list = []
    with torch.no_grad():
        for example in tqdm_notebook(self.data.test[:20000]):
            # numericalize query
            context = self.context.numericalize([example.context])
            citing = self.context.numericalize([example.authors_citing])
            context = context.to(DEVICE)
            citing = citing.to(DEVICE)

            # catch contexts and citings shorter than filter lengths and pad manually
            if context.shape[1] &lt; 5:
                assert self.pad == 1, &#34;Padding index doesn&#39;t match tensor index!&#34;
                padded = torch.ones((1,5), dtype=torch.long)
                padded[:, :context.shape[1]] = context
                context = padded.to(DEVICE)
            if citing.shape[1] &lt; 2:
                padded = torch.ones((1,2), dtype=torch.long)
                padded[:, :citing.shape[1]] = citing
                citing = padded.to(DEVICE)

            top_titles = self._get_bm_top(example.context)
            logger.info(f&#34;True title in top_titles: {example.title_cited in top_titles}.&#34;)
            top_authors = [self.title_aut_cited[tuple(title)] for title in top_titles]
            
            # add all true cited titles (can be multiple per context)
            indices = self.context_cited_indices[tuple(example.context)]
            append_count = 0
            for i in indices: 
                top_titles.append(self.examples[i].title_cited)
                top_authors.append(self.examples[i].authors_cited)
                append_count += 1

            

            logger.debug(f&#34;Number of candidate authors {len(top_authors)}.&#34;)
            logger.debug(f&#34;Number of candidate titles {len(top_titles)}.&#34;)
            assert len(top_authors) == len(top_titles), &#34;Evaluation title and author lengths don&#39;t match!&#34;

            # prepare batches
            citeds = self.authors.numericalize(self.authors.pad(top_authors))
            titles = self.title.numericalize(self.title.pad(top_titles))
            citeds = citeds.to(DEVICE)
            titles = titles.to(DEVICE)

            # repeat context and citing to len(indices) and calculate loss for single, large batch
            context = context.repeat(len(top_titles), 1)
            citing = citing.repeat(len(top_titles), 1)
            msg = &#34;Evaluation batch sizes don&#39;t match!&#34;
            assert context.shape[0] == citing.shape[0] == citeds.shape[0] == titles.shape[1], msg

            logger.debug(f&#34;Context shape: {context.shape}.&#34;)
            logger.debug(f&#34;Citing shape: {citing.shape}.&#34;)
            logger.debug(f&#34;Titles shape: {titles.shape}.&#34;)
            logger.debug(f&#34;Citeds shape: {citeds.shape}.&#34;)

            # calculate scores
            output = self.model(context = context, title = titles, authors_citing = citing, authors_cited = citeds)
            output = output[1:].permute(1,2,0)
            titles = titles[1:].permute(1,0)

            logger.debug(f&#34;Evaluation output shapes: {output.shape}&#34;)
            logger.debug(f&#34;Evaluation title shapes: {titles.shape}&#34;)

            scores = self.criterion(output, titles)
            scores = scores.sum(dim=1)
            logger.info(f&#34;Evaluation scores shape: {scores.shape}&#34;)

            _, index = scores.topk(x, largest=False, sorted=True, dim=0)

            logger.info(f&#34;Index: {index}&#34;)
            logger.info(f&#34;Lowest scores: {scores[index]}&#34;)
            logger.info(f&#34;Range of true titles: {len(top_titles) - 1} - {len(top_titles) - 1 - append_count}&#34;)

            # check how many of the concatenated (=true) titles have been returned
            scored = 0
            for i in range(append_count):
                if len(top_titles) - (i + 1) in index: scored += 1
                
            logger.info(f&#34;Scored {scored} out of {append_count} titles at {x}.&#34;)
            
            recall_list.append(scored/append_count)

        return sum(recall_list) / len(self.data.test[:20000])</code></pre>
</details>
</dd>
<dt id="ncn.evaluation.Evaluator.recommend"><code class="name flex">
<span>def <span class="ident">recommend</span></span>(<span>self, query: Union[str, List[str]], citing: Union[str, List[str]], top_x: int = 5)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def recommend(self, query: Stringlike, citing: Stringlike, top_x: int = 5):
    if self.eval: warnings.warn(&#34;Performing inference only on the test set.&#34;, RuntimeWarning)
    
    if isinstance(query, str): 
        query = self.context.tokenize(query)
    if isinstance(citing, str):
        citing = self.authors.tokenize(citing)
     
    with torch.no_grad():
        top_titles = self._get_bm_top(query)
        top_authors = [self.title_aut_cited[tuple(title)] for title in top_titles]
        assert len(top_authors) == len(top_titles), &#34;Evaluation title and author lengths don&#39;t match!&#34;

        context = self.context.numericalize([query])
        citing = self.context.numericalize([citing])
        context = context.to(DEVICE)
        citing = citing.to(DEVICE)

        # prepare batches
        citeds = self.authors.numericalize(self.authors.pad(top_authors))
        titles = self.title.numericalize(self.title.pad(top_titles))
        citeds = citeds.to(DEVICE)
        titles = titles.to(DEVICE)

        logger.debug(f&#34;Evaluation title shapes: {titles.shape}&#34;)

        # repeat context and citing to len(indices) and calculate loss for single, large batch
        context = context.repeat(len(top_titles), 1)
        citing = citing.repeat(len(top_titles), 1)
        msg = &#34;Evaluation batch sizes don&#39;t match!&#34;
        assert context.shape[0] == citing.shape[0] == citeds.shape[0] == titles.shape[1], msg

        # calculate scores
        if self.show_attention:
            output, attention = self.model(context = context, title = titles, 
                                           authors_citing = citing, authors_cited = citeds)
        else:
            output = self.model(context = context, title = titles, 
                                authors_citing = citing, authors_cited = citeds)
        output = output[1:].permute(1,2,0)
        titles = titles[1:].permute(1,0)

        logger.debug(f&#34;Evaluation output shapes: {output.shape}&#34;)
        logger.debug(f&#34;Evaluation title shapes: {titles.shape}&#34;)

        scores = self.criterion(output, titles)
        scores = scores.sum(dim=1)
        logger.debug(f&#34;Evaluation scores shape: {scores.shape}&#34;)
        _, index = scores.topk(top_x, largest=False, sorted=True, dim=0)

        recommended = [&#34; &#34;.join(top_titles[i]) for i in index]
    
    if self.show_attention:
        return {i: self.title_to_full[title] for i, title in enumerate(recommended)}, attention[:, index, :]

    return {i: self.title_to_full[title] for i, title in enumerate(recommended)}</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ncn" href="index.html">ncn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ncn.evaluation.Evaluator" href="#ncn.evaluation.Evaluator">Evaluator</a></code></h4>
<ul class="">
<li><code><a title="ncn.evaluation.Evaluator.recall" href="#ncn.evaluation.Evaluator.recall">recall</a></code></li>
<li><code><a title="ncn.evaluation.Evaluator.recommend" href="#ncn.evaluation.Evaluator.recommend">recommend</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>