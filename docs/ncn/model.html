<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>ncn.model API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ncn.model</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import logging
import random
from typing import List, Tuple

import torch
from torch import nn
import torch.nn.functional as F
from torch import Tensor

import ncn.core
from ncn.core import Filters, DEVICE

logger = logging.getLogger(&#34;neural_citation.ncn&#34;)


class TDNN(nn.Module):
    &#34;&#34;&#34;
    Single TDNN Block for the neural citation network.
    Implementation is based on:  
    https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf.  
    Consists of the following layers (in order): Convolution, ReLu, Batchnorm, MaxPool.  

    ## Parameters:   

    - **filter_size** *(int)*: filter length for the convolutional operation  
    - **embed_size** *(int)*: Dimension of the input word embeddings  
    - **num_filters** *(int)*: Number of convolutional filters  
    &#34;&#34;&#34;

    def __init__(self, filter_size: int, 
                       embed_size: int, 
                       num_filters: int):
        super().__init__()
        # model input shape: [N: batch size, D: embedding dimensions, L: sequence length]
        # no bias to avoid accumulating biases on padding
        self.conv = nn.Conv2d(1, num_filters, kernel_size=(embed_size, filter_size), bias=False)

    def forward(self, x: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;
        ## Input:  

        - **Embedded sequence** *(batch size, seq length, embedding dimensions)*:  
            Tensor containing a batch of embedded input sequences.

        ## Output:  

        - **Convolved sequence** *(batch_size, num_filters)*:  
            Tensor containing the output. 
        &#34;&#34;&#34;
        # [N: batch size, L: seq length, D embedding dimensions] -&gt; [N: batch size, D embedding dimensions, L: seq length]
        x = x.permute(0, 2, 1)
        # output shape: [N: batch size, 1: channels, D: embedding dimensions, L: sequence length]
        x = x.unsqueeze(1)


        # output shape: batch_size, num_filters, 1, f(seq length)
        x = F.relu(self.conv(x))
        pool_size = x.shape[-1]

        # output shape: batch_size, num_filters, 1, 1
        x = F.max_pool2d(x, kernel_size=pool_size)

        # output shape: batch_size, 1, num_filters, 1
        return x.permute(0, 2, 1, 3)


class TDNNEncoder(nn.Module):
    &#34;&#34;&#34;
    Encoder Module based on the TDNN architecture.
    Applies as list of filters with different region sizes on an input sequence.  
    The resulting feature maps are then allowed to interact with each other across a fully connected layer.  
    
    ## Parameters:  
    
    - **filters** *(Filters)*: List of integers determining the filter lengths.    
    - **num_filters** *(int)*: Number of filters applied in the TDNN convolutional layers.  
    - **embed_size** *(int)*: Dimensions of the used embeddings.  
    &#34;&#34;&#34;
    def __init__(self, filters: Filters,
                       num_filters: int,
                       embed_size: int):

        super().__init__()
        self.filter_list = filters
        self.num_filters = num_filters
        self._num_filters_total = len(filters)*num_filters

        self.encoder = nn.ModuleList([TDNN(filter_size=f, embed_size = embed_size, num_filters=num_filters).to(DEVICE) 
                                        for f in self.filter_list])
        self.fc = nn.Linear(self._num_filters_total, self._num_filters_total)


    def forward(self, x: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;
        ## Input:  

        - **Embeddings** *(batch size, seq length, embedding dimensions)*:
            Embedded input sequence.  

        ## Output:  

        - **Encodings** *(number of filter sizes, batch size, # filters)*:
            Tensor containing the complete context/author encodings.
        &#34;&#34;&#34;
        x = [encoder(x) for encoder in self.encoder]
        assert len(set([e.shape[0] for e in x])) == 1, &#34;Batch sizes don&#39;t match!&#34;


        # output shape: batch_size, list_length, num_filters
        x = torch.cat(x, dim=1).squeeze(3)

        batch_size = x.shape[0]

        # output shape: batch_size, list_length*num_filters
        x = x.view(batch_size, -1)

        # apply nonlinear mapping
        x = torch.tanh(self.fc(x))

        # output shape: list_length, batch_size, num_filters
        return x.view(len(self.filter_list), -1, self.num_filters)



class NCNEncoder(nn.Module):
    &#34;&#34;&#34;
    Encoder for the NCN model. Initializes TDNN Encoders for context and authors and concatenates the output.    
    
    ## Parameters:  
    - **context_filters** *(int)*: List of ints representing the context filter lengths.  
    - **author_filters** *(int)*: List of ints representing the author filter lengths.  
    - **context_vocab_size** *(int)*: Size of the context vocabulary. Used to train context embeddings.  
    - **title_vocab_size** *(int)*: Size of the title vocabulary. Used to train title embeddings.  
    - **author_vocab_size** *(int)*: Size of the author vocabulary. Used to train author embeddings.  
    - **num_filters** *(int)*: Number of filters applied in the TDNN layers of the model.   
    - **embed_size** *(int)*: Dimension of the learned author, context and title embeddings.  
    - **pad_idx** *(int)*: Index of the pad token in the vocabulary. Is set to zeros by the embedding layer.   
    - **dropout_p** *(float)*: Dropout probability for the dropout regularization layers.  
    - **authors** *(bool)*: Use author information in the encoder.   
    &#34;&#34;&#34;
    def __init__(self, context_filters: Filters,
                       author_filters: Filters,
                       context_vocab_size: int,
                       author_vocab_size: int,
                       num_filters: int,
                       embed_size: int,
                       pad_idx: int,
                       dropout_p: float,
                       authors: bool):
        super().__init__()

        self.use_authors = authors

        self.dropout = nn.Dropout(dropout_p)

        # context encoder
        self.context_embedding = nn.Embedding(context_vocab_size, embed_size, padding_idx=pad_idx)
        self.context_encoder = TDNNEncoder(context_filters, num_filters, embed_size)

        # author encoder
        if self.use_authors:
            self.author_embedding = nn.Embedding(author_vocab_size, embed_size, padding_idx=pad_idx)

            self.citing_author_encoder = TDNNEncoder(author_filters, num_filters, embed_size)
            self.cited_author_encoder = TDNNEncoder(author_filters, num_filters, embed_size)

    def forward(self, context: Tensor, 
                authors_citing: Tensor = None, authors_cited: Tensor = None) -&gt; Tensor:
        &#34;&#34;&#34;
        ## Input:  
        
        - **context** *(batch size, seq_length)*: 
            Tensor containing a batch of context indices.  
        - **authors_citing=None** *(batch size, seq_length)*:
            Tensor containing a batch of citing author indices.  
        - **authors_cited=None** *(batch size, seq_length)*: 
            Tensor containing a batch of cited author indices.
        
        ## Output:  
        
        - **output** *(batch_size, total # of filters (authors, cntxt), embedding size)*: 
            If authors= True the output tensor contains the concatenated context and author encodings.
            Else the encoded context is returned.
        &#34;&#34;&#34;
        # Embed and encode context
        context = self.dropout(self.context_embedding(context))
        context = self.context_encoder(context)
        logger.debug(f&#34;Context encoding shape: {context.shape}&#34;)

        if self.use_authors and authors_citing is not None and authors_cited is not None:
            logger.debug(&#34;Forward pass uses author information.&#34;)

            # Embed authors in shared space
            authors_citing = self.dropout(self.author_embedding(authors_citing))
            authors_cited = self.dropout(self.author_embedding(authors_cited))

            # Encode author information and concatenate
            authors_citing = self.citing_author_encoder(authors_citing)
            authors_cited = self.cited_author_encoder(authors_cited)
            logger.debug(f&#34;Citing author encoding shape: {authors_citing.shape}&#34;)
            logger.debug(f&#34;Cited author encoding shape: {authors_cited.shape}&#34;)

            # [N: batch_size, F: total # of filters (authors, cntxt), D: embedding size]
            return torch.cat([context, authors_citing, authors_cited], dim=0)
        
        return context


class Attention(nn.Module):
    &#34;&#34;&#34;
    Bahndanau attention module as published in the paper https://arxiv.org/abs/1409.0473.
    The code is based on https://github.com/bentrevett/pytorch-seq2seq.  
    
    ## Parameters:  
    
    - **enc_num_filters** *(int)*: Number of filters used in the encoder.  
    - **dec_hid_dim** *(int)*: Dimensions of the decoder RNN layer hidden state.   
    &#34;&#34;&#34;
    def __init__(self, enc_num_filters: int , dec_hid_dim: int):
        super().__init__()
        
        self.enc_num_filters = enc_num_filters
        self.dec_hid_dim = dec_hid_dim
        
        self.attn = nn.Linear(enc_num_filters + dec_hid_dim, dec_hid_dim)
        self.v = nn.Parameter(torch.rand(dec_hid_dim))
    
    def forward(self, hidden: Tensor, encoder_outputs: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;
        ## Input:  
        
        - **hidden** *(batch_size, dec_hidden_dim)*: Hidden state of the decoder recurrent layer.  
        - **encoder_otuputs** *(number of filter sizes, batch size, # filters)*: 
            Encoded context and author information.  
        
        ## Output:  
        
        - **a** *(batch_size, number of filter sizes)*: 
            Tensor containing the attention weights for the encoded source data.
        &#34;&#34;&#34;
        
        batch_size = encoder_outputs.shape[1]
        src_len = encoder_outputs.shape[0]
        
        logger.debug(f&#34;Attention Batch size: {batch_size}&#34;)
        logger.debug(f&#34;Attention weights: {src_len}&#34;)
        
        # Use only the last hidden state in case of multiple layers, i.e. hidden[-1]
        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) 
        
        energy = energy.permute(0, 2, 1)
        v = self.v.repeat(batch_size, 1).unsqueeze(1)    
        attention = torch.bmm(v, energy).squeeze(1)
        
        return torch.softmax(attention, dim=1)


class Decoder(nn.Module):
    &#34;&#34;&#34;
    Attention decoder for a Seq2Seq model. Uses a GRU layer as recurrent unit.  
    The code is based on https://github.com/bentrevett/pytorch-seq2seq.  
    
    ## Parameters:  
    
    - **title_vocab_size** *(int)*: Size of the title vocabulary used in the embedding layer.  
    - **embed_size** *(int)*: Dimensions of the learned embeddings.  
    - **enc_num_filters *(int)*: Number of filters used in the TDNN convolution layer.  
    - **hidden_size** *(int)*: Specifies the dimensions of the hidden GRU layer state.  
    - **pad_idx** *(int)*: Index used for pad tokens. Will be ignored by the embedding layer.  
    - **dropout_p** *(float)*: Dropout probability.  
    - **num_layers** *(float)*: Number of GRU layers.  
    - **attention** *(nn.Module)*: Module for computing the attention weights.  
    - **show_attention** *(bool)*: If True, the decoder also returns the attention weight matrix.  
    &#34;&#34;&#34;
    def __init__(self, title_vocab_size: int, embed_size: int, enc_num_filters: int, hidden_size: int,
                 pad_idx: int, dropout_p: float, num_layers: int,
                 attention: nn.Module, show_attention: bool):
        super().__init__()

        self.num_layers = num_layers
        self.embed_size = embed_size
        self.enc_num_filtes = enc_num_filters
        self.hidden_size = hidden_size
        self.title_vocab_size = title_vocab_size
        self.dropout_p = dropout_p
        self.attention = attention
        self.show_attention = show_attention
        
        self.embedding = nn.Embedding(title_vocab_size, embed_size, padding_idx=pad_idx)
        self.rnn = nn.GRU(input_size=enc_num_filters + embed_size,
                          hidden_size=hidden_size,
                          num_layers=num_layers,
                          dropout=self.dropout_p)
        
        self.out = nn.Linear(enc_num_filters*2 + embed_size, title_vocab_size)
        
        self.dropout = nn.Dropout(dropout_p)
    
    
    def init_hidden(self, bs: int):
        &#34;&#34;&#34;Initializes the RNN hidden state to a tensor of zeros of appropriate size.&#34;&#34;&#34;
        return torch.zeros(self.num_layers, bs, self.hidden_size, device=DEVICE)
    
    def forward(self, title: Tensor, hidden: Tensor, encoder_outputs: Tensor) -&gt; Tuple[Tensor, ...]:
        &#34;&#34;&#34;
        ## Input:  
        
        - **title** *(batch size)*: Batch of initial title tokens.  
        - **hidden** *(batch size, hidden_dim): Hidden state of the recurrent unit.  
        - **encoder_otuputs** *(number of filter sizes, batch size, # filters)*: 
            Encoded context and author information. 
        
        ## Output:  
        
        - **output** *(batch size, vocab_size)*: Scores for each word in the vocab.  
        - **hidden** *(batch size, hidden_dim): Hidden state of the recurrent unit.  
        &#34;&#34;&#34;
        
        input = title.unsqueeze(0)
        logger.debug(f&#34;Title shape: {title.shape}&#34;)
        logger.debug(f&#34;Hidden shape: {hidden.shape}&#34;)
        logger.debug(f&#34;Encoder output shape: {encoder_outputs.shape}&#34;)
        
        embedded = self.dropout(self.embedding(input))
        logger.debug(f&#34;Embedded shape: {embedded.shape}&#34;)
         
        a = self.attention(hidden, encoder_outputs)
        a = a.unsqueeze(1)
        logger.debug(f&#34;Attention output shape: {a.shape}&#34;)
        logger.debug(f&#34;Encoder outputs: {encoder_outputs.shape}&#34;)
        
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        weighted = torch.bmm(a, encoder_outputs)
        weighted = weighted.permute(1, 0, 2)
        logger.debug(f&#34;Weighted shape: {weighted.shape}&#34;)
        
        rnn_input = torch.cat((embedded, weighted), dim = 2)
        logger.debug(f&#34;RNN input shape: {rnn_input.shape}&#34;)  
        output, hidden = self.rnn(rnn_input, hidden)
        
        embedded = embedded.squeeze(0)
        output = output.squeeze(0)
        weighted = weighted.squeeze(0)
        logger.debug(f&#34;Hidden shape: {hidden.shape}&#34;)
        logger.debug(f&#34;Decoder Output shape: {output.shape}&#34;)
        logger.debug(f&#34;Weighted shape: {weighted.shape}&#34;)
        
        output = self.out(torch.cat((output, weighted, embedded), dim = 1))
        
        if self.show_attention:
            return output, hidden, a.squeeze(1)

        return output, hidden



class NeuralCitationNetwork(nn.Module):
    &#34;&#34;&#34;
    PyTorch implementation of the neural citation network by Ebesu &amp; Fang.  
    The original paper can be found here:  
    http://www.cse.scu.edu/~yfang/NCN.pdf.   
    The author&#39;s tensorflow code is on github:  
    https://github.com/tebesu/NeuralCitationNetwork.  

    ## Parameters:  
    - **context_filters** *(Filters)*: List of ints representing the context filter lengths.  
    - **author_filters** *(Filters)*: List of ints representing the author filter lengths.  
    - **context_vocab_size** *(int)*: Size of the context vocabulary. Used to train context embeddings.  
    - **title_vocab_size** *(int)*: Size of the title vocabulary. Used to train title embeddings.  
    - **author_vocab_size** *(int)*: Size of the author vocabulary. Used to train author embeddings.  
    - **pad_idx** *(int)*: Index of the pad token in the vocabulary. Is set to zeros by the embedding layer.   
    - **num_filters** *(int)*: Number of filters applied in the TDNN layers of the model.   
    - **authors** *(bool)*: Use author information in the encoder.  
    - **embed_size** *(int)*: Dimension of the learned author, context and title embeddings.  
    - **num_layers** *(int)*: Number of recurrent layers.  
    - **hidden_size** *(int)*: Dimension of the recurrent unit hidden states.  
    - **dropout_p** *(float=0.2)*: Dropout probability for the dropout regularization layers.  
    - **show_attention** *(bool=false)*: Returns attention tensors if true.  
    &#34;&#34;&#34;
    def __init__(self, context_filters: Filters,
                       author_filters: Filters,
                       context_vocab_size: int,
                       title_vocab_size: int,
                       author_vocab_size: int,
                       pad_idx: int,
                       num_filters: int,
                       authors: bool, 
                       embed_size: int,
                       num_layers: int, 
                       hidden_size: int,
                       dropout_p: float = 0.2,
                       show_attention: bool = False):
        super().__init__()


        self.use_authors = authors
        self.context_filter_list = context_filters
        self.author_filter_list = author_filters
        self.num_filters = num_filters # num filters for context == num filters for authors

        self.embed_size = embed_size
        self.context_vocab_size = context_vocab_size
        self.title_vocab_size = title_vocab_size
        self.author_vocab_size = author_vocab_size
        self.pad_idx = pad_idx

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.dropout_p = dropout_p
        self.show_attention = show_attention

        #---------------------------------------------------------------------------------------------------------------
        # NCN MODEL
        
        # Encoder
        self.encoder = NCNEncoder(context_filters = self.context_filter_list,
                                  author_filters = self.author_filter_list,
                                  context_vocab_size = self.context_vocab_size,
                                  author_vocab_size = self.author_vocab_size,
                                  num_filters = self.num_filters,
                                  embed_size = self.embed_size,
                                  pad_idx = self.pad_idx,
                                  dropout_p= self.dropout_p,
                                  authors = self.use_authors)

        # attention decoder
        self.attention = Attention(self.num_filters , self.hidden_size)
        self.decoder = Decoder(title_vocab_size = self.title_vocab_size,
                               embed_size = self.embed_size,
                               enc_num_filters = self.num_filters,
                               hidden_size = self.hidden_size,
                               pad_idx = self.pad_idx,
                               dropout_p = self.dropout_p,
                               num_layers = self.num_layers,
                               attention = self.attention,
                               show_attention=self.show_attention)
        

        self.settings = (
            f&#34;INITIALIZING NEURAL CITATION NETWORK WITH AUTHORS = {self.use_authors}&#34;
            f&#34;\nRunning on: {DEVICE}&#34;
            f&#34;\nNumber of model parameters: {self.count_parameters():,}&#34;
            f&#34;\nEncoders: # Filters = {self.num_filters}, &#34;
            f&#34;Context filter length = {self.context_filter_list},  Context filter length = {self.author_filter_list}&#34;
            f&#34;\nEmbeddings: Dimension = {self.embed_size}, Pad index = {self.pad_idx}, Context vocab = {self.context_vocab_size}, &#34;
            f&#34;Author vocab = {self.author_vocab_size}, Title vocab = {self.title_vocab_size}&#34;
            f&#34;\nDecoder: # GRU cells = {self.num_layers}, Hidden size = {self.hidden_size}&#34;
            f&#34;\nParameters: Dropout = {self.dropout_p}, Show attention = {self.show_attention}&#34;
            &#34;\n-------------------------------------------------&#34;
        )

    def count_parameters(self):
        &#34;&#34;&#34;Calculates the number of trainable parameters.&#34;&#34;&#34; 
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

    def forward(self, context: Tensor, title: Tensor, 
                authors_citing: Tensor = None, authors_cited: Tensor = None,
                teacher_forcing_ratio: float = 1):
        &#34;&#34;&#34;
        ## Parameters:  

        - **teacher_forcing_ratio** *(float=1)*: Determines the ratio with which
            the model is fed the true output to predict the next token. Defaults to 1 which means
            a token is always conditioned on the true previous output.

        ## Inputs:  
    
        - **context** *(batch size, seq_length)*: 
            Tensor containing a batch of context indices.  
        - **title** *(seq_length, batch size)*: 
            Tensor containing a batch of title indices. Note: not batch first!
        - **authors_citing=None** *(batch size, seq_length):
            Tensor containing a batch of citing author indices.  
        - **authors_cited=None** *(batch size, seq_length)*: 
            Tensor containing a batch of cited author indices. 
        
        ## Output:  
        
        - **output** *(batch_size, seq_len, title_vocab_len)*: 
            Tensor containing the predictions of the decoder.
         **attentions** *(batch_size, title_vocab_len)*: 
            Tensor containing the decoder attention states.
        &#34;&#34;&#34;
        
        encoder_outputs = self.encoder(context, authors_citing, authors_cited)
        
        batch_size = title.shape[1]
        max_len = title.shape[0]
        
        #tensor to store decoder outputs
        outputs = torch.zeros(max_len, batch_size, self.title_vocab_size).to(DEVICE)   
        #first input to the decoder is the &lt;sos&gt; tokens
        output = title[0,:]

        if self.show_attention:
            attentions = torch.zeros((max_len, batch_size, encoder_outputs.shape[0])).to(DEVICE)
            logger.debug(f&#34;Attentions viz shape: {attentions.shape}&#34;)
        
        hidden= self.decoder.init_hidden(batch_size)
        
        for t in range(1, max_len):
            if self.show_attention:
                output, hidden, attention = self.decoder(output, hidden, encoder_outputs)
                logger.debug(f&#34;Attentions output shape: {attention.shape}&#34;)
                attentions[t] = attention
            else:
                output, hidden = self.decoder(output, hidden, encoder_outputs)
            outputs[t] = output
            teacher_force = random.random() &lt; teacher_forcing_ratio
            top1 = output.max(1)[1]
            output = (title[t] if teacher_force else top1)

        logger.debug(f&#34;Model output shape: {outputs.shape}&#34;)

        if self.show_attention:
            return outputs, attentions
        
        return outputs</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ncn.model.Attention"><code class="flex name class">
<span>class <span class="ident">Attention</span></span>
<span>(</span><span>enc_num_filters: int, dec_hid_dim: int)</span>
</code></dt>
<dd>
<section class="desc"><p>Bahndanau attention module as published in the paper <a href="https://arxiv.org/abs/1409.0473.">https://arxiv.org/abs/1409.0473.</a>
The code is based on <a href="https://github.com/bentrevett/pytorch-seq2seq.">https://github.com/bentrevett/pytorch-seq2seq.</a>
</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li><strong>enc_num_filters</strong> <em>(int)</em>: Number of filters used in the encoder.
</li>
<li><strong>dec_hid_dim</strong> <em>(int)</em>: Dimensions of the decoder RNN layer hidden state.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Attention(nn.Module):
    &#34;&#34;&#34;
    Bahndanau attention module as published in the paper https://arxiv.org/abs/1409.0473.
    The code is based on https://github.com/bentrevett/pytorch-seq2seq.  
    
    ## Parameters:  
    
    - **enc_num_filters** *(int)*: Number of filters used in the encoder.  
    - **dec_hid_dim** *(int)*: Dimensions of the decoder RNN layer hidden state.   
    &#34;&#34;&#34;
    def __init__(self, enc_num_filters: int , dec_hid_dim: int):
        super().__init__()
        
        self.enc_num_filters = enc_num_filters
        self.dec_hid_dim = dec_hid_dim
        
        self.attn = nn.Linear(enc_num_filters + dec_hid_dim, dec_hid_dim)
        self.v = nn.Parameter(torch.rand(dec_hid_dim))
    
    def forward(self, hidden: Tensor, encoder_outputs: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;
        ## Input:  
        
        - **hidden** *(batch_size, dec_hidden_dim)*: Hidden state of the decoder recurrent layer.  
        - **encoder_otuputs** *(number of filter sizes, batch size, # filters)*: 
            Encoded context and author information.  
        
        ## Output:  
        
        - **a** *(batch_size, number of filter sizes)*: 
            Tensor containing the attention weights for the encoded source data.
        &#34;&#34;&#34;
        
        batch_size = encoder_outputs.shape[1]
        src_len = encoder_outputs.shape[0]
        
        logger.debug(f&#34;Attention Batch size: {batch_size}&#34;)
        logger.debug(f&#34;Attention weights: {src_len}&#34;)
        
        # Use only the last hidden state in case of multiple layers, i.e. hidden[-1]
        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) 
        
        energy = energy.permute(0, 2, 1)
        v = self.v.repeat(batch_size, 1).unsqueeze(1)    
        attention = torch.bmm(v, energy).squeeze(1)
        
        return torch.softmax(attention, dim=1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ncn.model.Attention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, hidden: torch.Tensor, encoder_outputs: torch.Tensor) -> torch.Tensor</span>
</code></dt>
<dd>
<section class="desc"><h2 id="input">Input:</h2>
<ul>
<li><strong>hidden</strong> <em>(batch_size, dec_hidden_dim)</em>: Hidden state of the decoder recurrent layer.
</li>
<li><strong>encoder_otuputs</strong> <em>(number of filter sizes, batch size, # filters)</em>:
Encoded context and author information.
</li>
</ul>
<h2 id="output">Output:</h2>
<ul>
<li><strong>a</strong> <em>(batch_size, number of filter sizes)</em>:
Tensor containing the attention weights for the encoded source data.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, hidden: Tensor, encoder_outputs: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    ## Input:  
    
    - **hidden** *(batch_size, dec_hidden_dim)*: Hidden state of the decoder recurrent layer.  
    - **encoder_otuputs** *(number of filter sizes, batch size, # filters)*: 
        Encoded context and author information.  
    
    ## Output:  
    
    - **a** *(batch_size, number of filter sizes)*: 
        Tensor containing the attention weights for the encoded source data.
    &#34;&#34;&#34;
    
    batch_size = encoder_outputs.shape[1]
    src_len = encoder_outputs.shape[0]
    
    logger.debug(f&#34;Attention Batch size: {batch_size}&#34;)
    logger.debug(f&#34;Attention weights: {src_len}&#34;)
    
    # Use only the last hidden state in case of multiple layers, i.e. hidden[-1]
    hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)
    encoder_outputs = encoder_outputs.permute(1, 0, 2)
    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) 
    
    energy = energy.permute(0, 2, 1)
    v = self.v.repeat(batch_size, 1).unsqueeze(1)    
    attention = torch.bmm(v, energy).squeeze(1)
    
    return torch.softmax(attention, dim=1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ncn.model.Decoder"><code class="flex name class">
<span>class <span class="ident">Decoder</span></span>
<span>(</span><span>title_vocab_size: int, embed_size: int, enc_num_filters: int, hidden_size: int, pad_idx: int, dropout_p: float, num_layers: int, attention: torch.nn.modules.module.Module, show_attention: bool)</span>
</code></dt>
<dd>
<section class="desc"><p>Attention decoder for a Seq2Seq model. Uses a GRU layer as recurrent unit.<br>
The code is based on <a href="https://github.com/bentrevett/pytorch-seq2seq.">https://github.com/bentrevett/pytorch-seq2seq.</a>
</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li><strong>title_vocab_size</strong> <em>(int)</em>: Size of the title vocabulary used in the embedding layer.
</li>
<li><strong>embed_size</strong> <em>(int)</em>: Dimensions of the learned embeddings.
</li>
<li><em><em>enc_num_filters </em>(int)</em>: Number of filters used in the TDNN convolution layer.
</li>
<li><strong>hidden_size</strong> <em>(int)</em>: Specifies the dimensions of the hidden GRU layer state.
</li>
<li><strong>pad_idx</strong> <em>(int)</em>: Index used for pad tokens. Will be ignored by the embedding layer.
</li>
<li><strong>dropout_p</strong> <em>(float)</em>: Dropout probability.
</li>
<li><strong>num_layers</strong> <em>(float)</em>: Number of GRU layers.
</li>
<li><strong>attention</strong> <em>(nn.Module)</em>: Module for computing the attention weights.
</li>
<li><strong>show_attention</strong> <em>(bool)</em>: If True, the decoder also returns the attention weight matrix.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Decoder(nn.Module):
    &#34;&#34;&#34;
    Attention decoder for a Seq2Seq model. Uses a GRU layer as recurrent unit.  
    The code is based on https://github.com/bentrevett/pytorch-seq2seq.  
    
    ## Parameters:  
    
    - **title_vocab_size** *(int)*: Size of the title vocabulary used in the embedding layer.  
    - **embed_size** *(int)*: Dimensions of the learned embeddings.  
    - **enc_num_filters *(int)*: Number of filters used in the TDNN convolution layer.  
    - **hidden_size** *(int)*: Specifies the dimensions of the hidden GRU layer state.  
    - **pad_idx** *(int)*: Index used for pad tokens. Will be ignored by the embedding layer.  
    - **dropout_p** *(float)*: Dropout probability.  
    - **num_layers** *(float)*: Number of GRU layers.  
    - **attention** *(nn.Module)*: Module for computing the attention weights.  
    - **show_attention** *(bool)*: If True, the decoder also returns the attention weight matrix.  
    &#34;&#34;&#34;
    def __init__(self, title_vocab_size: int, embed_size: int, enc_num_filters: int, hidden_size: int,
                 pad_idx: int, dropout_p: float, num_layers: int,
                 attention: nn.Module, show_attention: bool):
        super().__init__()

        self.num_layers = num_layers
        self.embed_size = embed_size
        self.enc_num_filtes = enc_num_filters
        self.hidden_size = hidden_size
        self.title_vocab_size = title_vocab_size
        self.dropout_p = dropout_p
        self.attention = attention
        self.show_attention = show_attention
        
        self.embedding = nn.Embedding(title_vocab_size, embed_size, padding_idx=pad_idx)
        self.rnn = nn.GRU(input_size=enc_num_filters + embed_size,
                          hidden_size=hidden_size,
                          num_layers=num_layers,
                          dropout=self.dropout_p)
        
        self.out = nn.Linear(enc_num_filters*2 + embed_size, title_vocab_size)
        
        self.dropout = nn.Dropout(dropout_p)
    
    
    def init_hidden(self, bs: int):
        &#34;&#34;&#34;Initializes the RNN hidden state to a tensor of zeros of appropriate size.&#34;&#34;&#34;
        return torch.zeros(self.num_layers, bs, self.hidden_size, device=DEVICE)
    
    def forward(self, title: Tensor, hidden: Tensor, encoder_outputs: Tensor) -&gt; Tuple[Tensor, ...]:
        &#34;&#34;&#34;
        ## Input:  
        
        - **title** *(batch size)*: Batch of initial title tokens.  
        - **hidden** *(batch size, hidden_dim): Hidden state of the recurrent unit.  
        - **encoder_otuputs** *(number of filter sizes, batch size, # filters)*: 
            Encoded context and author information. 
        
        ## Output:  
        
        - **output** *(batch size, vocab_size)*: Scores for each word in the vocab.  
        - **hidden** *(batch size, hidden_dim): Hidden state of the recurrent unit.  
        &#34;&#34;&#34;
        
        input = title.unsqueeze(0)
        logger.debug(f&#34;Title shape: {title.shape}&#34;)
        logger.debug(f&#34;Hidden shape: {hidden.shape}&#34;)
        logger.debug(f&#34;Encoder output shape: {encoder_outputs.shape}&#34;)
        
        embedded = self.dropout(self.embedding(input))
        logger.debug(f&#34;Embedded shape: {embedded.shape}&#34;)
         
        a = self.attention(hidden, encoder_outputs)
        a = a.unsqueeze(1)
        logger.debug(f&#34;Attention output shape: {a.shape}&#34;)
        logger.debug(f&#34;Encoder outputs: {encoder_outputs.shape}&#34;)
        
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        weighted = torch.bmm(a, encoder_outputs)
        weighted = weighted.permute(1, 0, 2)
        logger.debug(f&#34;Weighted shape: {weighted.shape}&#34;)
        
        rnn_input = torch.cat((embedded, weighted), dim = 2)
        logger.debug(f&#34;RNN input shape: {rnn_input.shape}&#34;)  
        output, hidden = self.rnn(rnn_input, hidden)
        
        embedded = embedded.squeeze(0)
        output = output.squeeze(0)
        weighted = weighted.squeeze(0)
        logger.debug(f&#34;Hidden shape: {hidden.shape}&#34;)
        logger.debug(f&#34;Decoder Output shape: {output.shape}&#34;)
        logger.debug(f&#34;Weighted shape: {weighted.shape}&#34;)
        
        output = self.out(torch.cat((output, weighted, embedded), dim = 1))
        
        if self.show_attention:
            return output, hidden, a.squeeze(1)

        return output, hidden</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ncn.model.Decoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, title: torch.Tensor, hidden: torch.Tensor, encoder_outputs: torch.Tensor) -> Tuple[torch.Tensor, ...]</span>
</code></dt>
<dd>
<section class="desc"><h2 id="input">Input:</h2>
<ul>
<li><strong>title</strong> <em>(batch size)</em>: Batch of initial title tokens.
</li>
<li><strong>hidden</strong> *(batch size, hidden_dim): Hidden state of the recurrent unit.
</li>
<li><strong>encoder_otuputs</strong> <em>(number of filter sizes, batch size, # filters)</em>:
Encoded context and author information. </li>
</ul>
<h2 id="output">Output:</h2>
<ul>
<li><strong>output</strong> <em>(batch size, vocab_size)</em>: Scores for each word in the vocab.
</li>
<li><strong>hidden</strong> *(batch size, hidden_dim): Hidden state of the recurrent unit.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, title: Tensor, hidden: Tensor, encoder_outputs: Tensor) -&gt; Tuple[Tensor, ...]:
    &#34;&#34;&#34;
    ## Input:  
    
    - **title** *(batch size)*: Batch of initial title tokens.  
    - **hidden** *(batch size, hidden_dim): Hidden state of the recurrent unit.  
    - **encoder_otuputs** *(number of filter sizes, batch size, # filters)*: 
        Encoded context and author information. 
    
    ## Output:  
    
    - **output** *(batch size, vocab_size)*: Scores for each word in the vocab.  
    - **hidden** *(batch size, hidden_dim): Hidden state of the recurrent unit.  
    &#34;&#34;&#34;
    
    input = title.unsqueeze(0)
    logger.debug(f&#34;Title shape: {title.shape}&#34;)
    logger.debug(f&#34;Hidden shape: {hidden.shape}&#34;)
    logger.debug(f&#34;Encoder output shape: {encoder_outputs.shape}&#34;)
    
    embedded = self.dropout(self.embedding(input))
    logger.debug(f&#34;Embedded shape: {embedded.shape}&#34;)
     
    a = self.attention(hidden, encoder_outputs)
    a = a.unsqueeze(1)
    logger.debug(f&#34;Attention output shape: {a.shape}&#34;)
    logger.debug(f&#34;Encoder outputs: {encoder_outputs.shape}&#34;)
    
    encoder_outputs = encoder_outputs.permute(1, 0, 2)
    weighted = torch.bmm(a, encoder_outputs)
    weighted = weighted.permute(1, 0, 2)
    logger.debug(f&#34;Weighted shape: {weighted.shape}&#34;)
    
    rnn_input = torch.cat((embedded, weighted), dim = 2)
    logger.debug(f&#34;RNN input shape: {rnn_input.shape}&#34;)  
    output, hidden = self.rnn(rnn_input, hidden)
    
    embedded = embedded.squeeze(0)
    output = output.squeeze(0)
    weighted = weighted.squeeze(0)
    logger.debug(f&#34;Hidden shape: {hidden.shape}&#34;)
    logger.debug(f&#34;Decoder Output shape: {output.shape}&#34;)
    logger.debug(f&#34;Weighted shape: {weighted.shape}&#34;)
    
    output = self.out(torch.cat((output, weighted, embedded), dim = 1))
    
    if self.show_attention:
        return output, hidden, a.squeeze(1)

    return output, hidden</code></pre>
</details>
</dd>
<dt id="ncn.model.Decoder.init_hidden"><code class="name flex">
<span>def <span class="ident">init_hidden</span></span>(<span>self, bs: int)</span>
</code></dt>
<dd>
<section class="desc"><p>Initializes the RNN hidden state to a tensor of zeros of appropriate size.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def init_hidden(self, bs: int):
    &#34;&#34;&#34;Initializes the RNN hidden state to a tensor of zeros of appropriate size.&#34;&#34;&#34;
    return torch.zeros(self.num_layers, bs, self.hidden_size, device=DEVICE)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ncn.model.NCNEncoder"><code class="flex name class">
<span>class <span class="ident">NCNEncoder</span></span>
<span>(</span><span>context_filters: List[int], author_filters: List[int], context_vocab_size: int, author_vocab_size: int, num_filters: int, embed_size: int, pad_idx: int, dropout_p: float, authors: bool)</span>
</code></dt>
<dd>
<section class="desc"><p>Encoder for the NCN model. Initializes TDNN Encoders for context and authors and concatenates the output.
</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li><strong>context_filters</strong> <em>(int)</em>: List of ints representing the context filter lengths.
</li>
<li><strong>author_filters</strong> <em>(int)</em>: List of ints representing the author filter lengths.
</li>
<li><strong>context_vocab_size</strong> <em>(int)</em>: Size of the context vocabulary. Used to train context embeddings.
</li>
<li><strong>title_vocab_size</strong> <em>(int)</em>: Size of the title vocabulary. Used to train title embeddings.
</li>
<li><strong>author_vocab_size</strong> <em>(int)</em>: Size of the author vocabulary. Used to train author embeddings.
</li>
<li><strong>num_filters</strong> <em>(int)</em>: Number of filters applied in the TDNN layers of the model.
</li>
<li><strong>embed_size</strong> <em>(int)</em>: Dimension of the learned author, context and title embeddings.
</li>
<li><strong>pad_idx</strong> <em>(int)</em>: Index of the pad token in the vocabulary. Is set to zeros by the embedding layer.
</li>
<li><strong>dropout_p</strong> <em>(float)</em>: Dropout probability for the dropout regularization layers.
</li>
<li><strong>authors</strong> <em>(bool)</em>: Use author information in the encoder.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class NCNEncoder(nn.Module):
    &#34;&#34;&#34;
    Encoder for the NCN model. Initializes TDNN Encoders for context and authors and concatenates the output.    
    
    ## Parameters:  
    - **context_filters** *(int)*: List of ints representing the context filter lengths.  
    - **author_filters** *(int)*: List of ints representing the author filter lengths.  
    - **context_vocab_size** *(int)*: Size of the context vocabulary. Used to train context embeddings.  
    - **title_vocab_size** *(int)*: Size of the title vocabulary. Used to train title embeddings.  
    - **author_vocab_size** *(int)*: Size of the author vocabulary. Used to train author embeddings.  
    - **num_filters** *(int)*: Number of filters applied in the TDNN layers of the model.   
    - **embed_size** *(int)*: Dimension of the learned author, context and title embeddings.  
    - **pad_idx** *(int)*: Index of the pad token in the vocabulary. Is set to zeros by the embedding layer.   
    - **dropout_p** *(float)*: Dropout probability for the dropout regularization layers.  
    - **authors** *(bool)*: Use author information in the encoder.   
    &#34;&#34;&#34;
    def __init__(self, context_filters: Filters,
                       author_filters: Filters,
                       context_vocab_size: int,
                       author_vocab_size: int,
                       num_filters: int,
                       embed_size: int,
                       pad_idx: int,
                       dropout_p: float,
                       authors: bool):
        super().__init__()

        self.use_authors = authors

        self.dropout = nn.Dropout(dropout_p)

        # context encoder
        self.context_embedding = nn.Embedding(context_vocab_size, embed_size, padding_idx=pad_idx)
        self.context_encoder = TDNNEncoder(context_filters, num_filters, embed_size)

        # author encoder
        if self.use_authors:
            self.author_embedding = nn.Embedding(author_vocab_size, embed_size, padding_idx=pad_idx)

            self.citing_author_encoder = TDNNEncoder(author_filters, num_filters, embed_size)
            self.cited_author_encoder = TDNNEncoder(author_filters, num_filters, embed_size)

    def forward(self, context: Tensor, 
                authors_citing: Tensor = None, authors_cited: Tensor = None) -&gt; Tensor:
        &#34;&#34;&#34;
        ## Input:  
        
        - **context** *(batch size, seq_length)*: 
            Tensor containing a batch of context indices.  
        - **authors_citing=None** *(batch size, seq_length)*:
            Tensor containing a batch of citing author indices.  
        - **authors_cited=None** *(batch size, seq_length)*: 
            Tensor containing a batch of cited author indices.
        
        ## Output:  
        
        - **output** *(batch_size, total # of filters (authors, cntxt), embedding size)*: 
            If authors= True the output tensor contains the concatenated context and author encodings.
            Else the encoded context is returned.
        &#34;&#34;&#34;
        # Embed and encode context
        context = self.dropout(self.context_embedding(context))
        context = self.context_encoder(context)
        logger.debug(f&#34;Context encoding shape: {context.shape}&#34;)

        if self.use_authors and authors_citing is not None and authors_cited is not None:
            logger.debug(&#34;Forward pass uses author information.&#34;)

            # Embed authors in shared space
            authors_citing = self.dropout(self.author_embedding(authors_citing))
            authors_cited = self.dropout(self.author_embedding(authors_cited))

            # Encode author information and concatenate
            authors_citing = self.citing_author_encoder(authors_citing)
            authors_cited = self.cited_author_encoder(authors_cited)
            logger.debug(f&#34;Citing author encoding shape: {authors_citing.shape}&#34;)
            logger.debug(f&#34;Cited author encoding shape: {authors_cited.shape}&#34;)

            # [N: batch_size, F: total # of filters (authors, cntxt), D: embedding size]
            return torch.cat([context, authors_citing, authors_cited], dim=0)
        
        return context</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ncn.model.NCNEncoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, context: torch.Tensor, authors_citing: torch.Tensor = None, authors_cited: torch.Tensor = None) -> torch.Tensor</span>
</code></dt>
<dd>
<section class="desc"><h2 id="input">Input:</h2>
<ul>
<li><strong>context</strong> <em>(batch size, seq_length)</em>:
Tensor containing a batch of context indices.
</li>
<li><strong>authors_citing=None</strong> <em>(batch size, seq_length)</em>:
Tensor containing a batch of citing author indices.
</li>
<li><strong>authors_cited=None</strong> <em>(batch size, seq_length)</em>:
Tensor containing a batch of cited author indices.</li>
</ul>
<h2 id="output">Output:</h2>
<ul>
<li><strong>output</strong> <em>(batch_size, total # of filters (authors, cntxt), embedding size)</em>:
If authors= True the output tensor contains the concatenated context and author encodings.
Else the encoded context is returned.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, context: Tensor, 
            authors_citing: Tensor = None, authors_cited: Tensor = None) -&gt; Tensor:
    &#34;&#34;&#34;
    ## Input:  
    
    - **context** *(batch size, seq_length)*: 
        Tensor containing a batch of context indices.  
    - **authors_citing=None** *(batch size, seq_length)*:
        Tensor containing a batch of citing author indices.  
    - **authors_cited=None** *(batch size, seq_length)*: 
        Tensor containing a batch of cited author indices.
    
    ## Output:  
    
    - **output** *(batch_size, total # of filters (authors, cntxt), embedding size)*: 
        If authors= True the output tensor contains the concatenated context and author encodings.
        Else the encoded context is returned.
    &#34;&#34;&#34;
    # Embed and encode context
    context = self.dropout(self.context_embedding(context))
    context = self.context_encoder(context)
    logger.debug(f&#34;Context encoding shape: {context.shape}&#34;)

    if self.use_authors and authors_citing is not None and authors_cited is not None:
        logger.debug(&#34;Forward pass uses author information.&#34;)

        # Embed authors in shared space
        authors_citing = self.dropout(self.author_embedding(authors_citing))
        authors_cited = self.dropout(self.author_embedding(authors_cited))

        # Encode author information and concatenate
        authors_citing = self.citing_author_encoder(authors_citing)
        authors_cited = self.cited_author_encoder(authors_cited)
        logger.debug(f&#34;Citing author encoding shape: {authors_citing.shape}&#34;)
        logger.debug(f&#34;Cited author encoding shape: {authors_cited.shape}&#34;)

        # [N: batch_size, F: total # of filters (authors, cntxt), D: embedding size]
        return torch.cat([context, authors_citing, authors_cited], dim=0)
    
    return context</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ncn.model.NeuralCitationNetwork"><code class="flex name class">
<span>class <span class="ident">NeuralCitationNetwork</span></span>
<span>(</span><span>context_filters: List[int], author_filters: List[int], context_vocab_size: int, title_vocab_size: int, author_vocab_size: int, pad_idx: int, num_filters: int, authors: bool, embed_size: int, num_layers: int, hidden_size: int, dropout_p: float = 0.2, show_attention: bool = False)</span>
</code></dt>
<dd>
<section class="desc"><p>PyTorch implementation of the neural citation network by Ebesu &amp; Fang.<br>
The original paper can be found here:<br>
<a href="http://www.cse.scu.edu/~yfang/NCN.pdf.">http://www.cse.scu.edu/~yfang/NCN.pdf.</a> <br>
The author's tensorflow code is on github:<br>
<a href="https://github.com/tebesu/NeuralCitationNetwork.">https://github.com/tebesu/NeuralCitationNetwork.</a>
</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li><strong>context_filters</strong> <em>(Filters)</em>: List of ints representing the context filter lengths.
</li>
<li><strong>author_filters</strong> <em>(Filters)</em>: List of ints representing the author filter lengths.
</li>
<li><strong>context_vocab_size</strong> <em>(int)</em>: Size of the context vocabulary. Used to train context embeddings.
</li>
<li><strong>title_vocab_size</strong> <em>(int)</em>: Size of the title vocabulary. Used to train title embeddings.
</li>
<li><strong>author_vocab_size</strong> <em>(int)</em>: Size of the author vocabulary. Used to train author embeddings.
</li>
<li><strong>pad_idx</strong> <em>(int)</em>: Index of the pad token in the vocabulary. Is set to zeros by the embedding layer.
</li>
<li><strong>num_filters</strong> <em>(int)</em>: Number of filters applied in the TDNN layers of the model.
</li>
<li><strong>authors</strong> <em>(bool)</em>: Use author information in the encoder.
</li>
<li><strong>embed_size</strong> <em>(int)</em>: Dimension of the learned author, context and title embeddings.
</li>
<li><strong>num_layers</strong> <em>(int)</em>: Number of recurrent layers.
</li>
<li><strong>hidden_size</strong> <em>(int)</em>: Dimension of the recurrent unit hidden states.
</li>
<li><strong>dropout_p</strong> <em>(float=0.2)</em>: Dropout probability for the dropout regularization layers.
</li>
<li><strong>show_attention</strong> <em>(bool=false)</em>: Returns attention tensors if true.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class NeuralCitationNetwork(nn.Module):
    &#34;&#34;&#34;
    PyTorch implementation of the neural citation network by Ebesu &amp; Fang.  
    The original paper can be found here:  
    http://www.cse.scu.edu/~yfang/NCN.pdf.   
    The author&#39;s tensorflow code is on github:  
    https://github.com/tebesu/NeuralCitationNetwork.  

    ## Parameters:  
    - **context_filters** *(Filters)*: List of ints representing the context filter lengths.  
    - **author_filters** *(Filters)*: List of ints representing the author filter lengths.  
    - **context_vocab_size** *(int)*: Size of the context vocabulary. Used to train context embeddings.  
    - **title_vocab_size** *(int)*: Size of the title vocabulary. Used to train title embeddings.  
    - **author_vocab_size** *(int)*: Size of the author vocabulary. Used to train author embeddings.  
    - **pad_idx** *(int)*: Index of the pad token in the vocabulary. Is set to zeros by the embedding layer.   
    - **num_filters** *(int)*: Number of filters applied in the TDNN layers of the model.   
    - **authors** *(bool)*: Use author information in the encoder.  
    - **embed_size** *(int)*: Dimension of the learned author, context and title embeddings.  
    - **num_layers** *(int)*: Number of recurrent layers.  
    - **hidden_size** *(int)*: Dimension of the recurrent unit hidden states.  
    - **dropout_p** *(float=0.2)*: Dropout probability for the dropout regularization layers.  
    - **show_attention** *(bool=false)*: Returns attention tensors if true.  
    &#34;&#34;&#34;
    def __init__(self, context_filters: Filters,
                       author_filters: Filters,
                       context_vocab_size: int,
                       title_vocab_size: int,
                       author_vocab_size: int,
                       pad_idx: int,
                       num_filters: int,
                       authors: bool, 
                       embed_size: int,
                       num_layers: int, 
                       hidden_size: int,
                       dropout_p: float = 0.2,
                       show_attention: bool = False):
        super().__init__()


        self.use_authors = authors
        self.context_filter_list = context_filters
        self.author_filter_list = author_filters
        self.num_filters = num_filters # num filters for context == num filters for authors

        self.embed_size = embed_size
        self.context_vocab_size = context_vocab_size
        self.title_vocab_size = title_vocab_size
        self.author_vocab_size = author_vocab_size
        self.pad_idx = pad_idx

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.dropout_p = dropout_p
        self.show_attention = show_attention

        #---------------------------------------------------------------------------------------------------------------
        # NCN MODEL
        
        # Encoder
        self.encoder = NCNEncoder(context_filters = self.context_filter_list,
                                  author_filters = self.author_filter_list,
                                  context_vocab_size = self.context_vocab_size,
                                  author_vocab_size = self.author_vocab_size,
                                  num_filters = self.num_filters,
                                  embed_size = self.embed_size,
                                  pad_idx = self.pad_idx,
                                  dropout_p= self.dropout_p,
                                  authors = self.use_authors)

        # attention decoder
        self.attention = Attention(self.num_filters , self.hidden_size)
        self.decoder = Decoder(title_vocab_size = self.title_vocab_size,
                               embed_size = self.embed_size,
                               enc_num_filters = self.num_filters,
                               hidden_size = self.hidden_size,
                               pad_idx = self.pad_idx,
                               dropout_p = self.dropout_p,
                               num_layers = self.num_layers,
                               attention = self.attention,
                               show_attention=self.show_attention)
        

        self.settings = (
            f&#34;INITIALIZING NEURAL CITATION NETWORK WITH AUTHORS = {self.use_authors}&#34;
            f&#34;\nRunning on: {DEVICE}&#34;
            f&#34;\nNumber of model parameters: {self.count_parameters():,}&#34;
            f&#34;\nEncoders: # Filters = {self.num_filters}, &#34;
            f&#34;Context filter length = {self.context_filter_list},  Context filter length = {self.author_filter_list}&#34;
            f&#34;\nEmbeddings: Dimension = {self.embed_size}, Pad index = {self.pad_idx}, Context vocab = {self.context_vocab_size}, &#34;
            f&#34;Author vocab = {self.author_vocab_size}, Title vocab = {self.title_vocab_size}&#34;
            f&#34;\nDecoder: # GRU cells = {self.num_layers}, Hidden size = {self.hidden_size}&#34;
            f&#34;\nParameters: Dropout = {self.dropout_p}, Show attention = {self.show_attention}&#34;
            &#34;\n-------------------------------------------------&#34;
        )

    def count_parameters(self):
        &#34;&#34;&#34;Calculates the number of trainable parameters.&#34;&#34;&#34; 
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

    def forward(self, context: Tensor, title: Tensor, 
                authors_citing: Tensor = None, authors_cited: Tensor = None,
                teacher_forcing_ratio: float = 1):
        &#34;&#34;&#34;
        ## Parameters:  

        - **teacher_forcing_ratio** *(float=1)*: Determines the ratio with which
            the model is fed the true output to predict the next token. Defaults to 1 which means
            a token is always conditioned on the true previous output.

        ## Inputs:  
    
        - **context** *(batch size, seq_length)*: 
            Tensor containing a batch of context indices.  
        - **title** *(seq_length, batch size)*: 
            Tensor containing a batch of title indices. Note: not batch first!
        - **authors_citing=None** *(batch size, seq_length):
            Tensor containing a batch of citing author indices.  
        - **authors_cited=None** *(batch size, seq_length)*: 
            Tensor containing a batch of cited author indices. 
        
        ## Output:  
        
        - **output** *(batch_size, seq_len, title_vocab_len)*: 
            Tensor containing the predictions of the decoder.
         **attentions** *(batch_size, title_vocab_len)*: 
            Tensor containing the decoder attention states.
        &#34;&#34;&#34;
        
        encoder_outputs = self.encoder(context, authors_citing, authors_cited)
        
        batch_size = title.shape[1]
        max_len = title.shape[0]
        
        #tensor to store decoder outputs
        outputs = torch.zeros(max_len, batch_size, self.title_vocab_size).to(DEVICE)   
        #first input to the decoder is the &lt;sos&gt; tokens
        output = title[0,:]

        if self.show_attention:
            attentions = torch.zeros((max_len, batch_size, encoder_outputs.shape[0])).to(DEVICE)
            logger.debug(f&#34;Attentions viz shape: {attentions.shape}&#34;)
        
        hidden= self.decoder.init_hidden(batch_size)
        
        for t in range(1, max_len):
            if self.show_attention:
                output, hidden, attention = self.decoder(output, hidden, encoder_outputs)
                logger.debug(f&#34;Attentions output shape: {attention.shape}&#34;)
                attentions[t] = attention
            else:
                output, hidden = self.decoder(output, hidden, encoder_outputs)
            outputs[t] = output
            teacher_force = random.random() &lt; teacher_forcing_ratio
            top1 = output.max(1)[1]
            output = (title[t] if teacher_force else top1)

        logger.debug(f&#34;Model output shape: {outputs.shape}&#34;)

        if self.show_attention:
            return outputs, attentions
        
        return outputs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ncn.model.NeuralCitationNetwork.count_parameters"><code class="name flex">
<span>def <span class="ident">count_parameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the number of trainable parameters.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def count_parameters(self):
    &#34;&#34;&#34;Calculates the number of trainable parameters.&#34;&#34;&#34; 
    return sum(p.numel() for p in self.parameters() if p.requires_grad)</code></pre>
</details>
</dd>
<dt id="ncn.model.NeuralCitationNetwork.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, context: torch.Tensor, title: torch.Tensor, authors_citing: torch.Tensor = None, authors_cited: torch.Tensor = None, teacher_forcing_ratio: float = 1)</span>
</code></dt>
<dd>
<section class="desc"><h2 id="parameters">Parameters:</h2>
<ul>
<li><strong>teacher_forcing_ratio</strong> <em>(float=1)</em>: Determines the ratio with which
the model is fed the true output to predict the next token. Defaults to 1 which means
a token is always conditioned on the true previous output.</li>
</ul>
<h2 id="inputs">Inputs:</h2>
<ul>
<li><strong>context</strong> <em>(batch size, seq_length)</em>:
Tensor containing a batch of context indices.
</li>
<li><strong>title</strong> <em>(seq_length, batch size)</em>:
Tensor containing a batch of title indices. Note: not batch first!</li>
<li><strong>authors_citing=None</strong> *(batch size, seq_length):
Tensor containing a batch of citing author indices.
</li>
<li><strong>authors_cited=None</strong> <em>(batch size, seq_length)</em>:
Tensor containing a batch of cited author indices. </li>
</ul>
<h2 id="output">Output:</h2>
<ul>
<li><strong>output</strong> <em>(batch_size, seq_len, title_vocab_len)</em>:
Tensor containing the predictions of the decoder.
<strong>attentions</strong> <em>(batch_size, title_vocab_len)</em>:
Tensor containing the decoder attention states.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, context: Tensor, title: Tensor, 
            authors_citing: Tensor = None, authors_cited: Tensor = None,
            teacher_forcing_ratio: float = 1):
    &#34;&#34;&#34;
    ## Parameters:  

    - **teacher_forcing_ratio** *(float=1)*: Determines the ratio with which
        the model is fed the true output to predict the next token. Defaults to 1 which means
        a token is always conditioned on the true previous output.

    ## Inputs:  

    - **context** *(batch size, seq_length)*: 
        Tensor containing a batch of context indices.  
    - **title** *(seq_length, batch size)*: 
        Tensor containing a batch of title indices. Note: not batch first!
    - **authors_citing=None** *(batch size, seq_length):
        Tensor containing a batch of citing author indices.  
    - **authors_cited=None** *(batch size, seq_length)*: 
        Tensor containing a batch of cited author indices. 
    
    ## Output:  
    
    - **output** *(batch_size, seq_len, title_vocab_len)*: 
        Tensor containing the predictions of the decoder.
     **attentions** *(batch_size, title_vocab_len)*: 
        Tensor containing the decoder attention states.
    &#34;&#34;&#34;
    
    encoder_outputs = self.encoder(context, authors_citing, authors_cited)
    
    batch_size = title.shape[1]
    max_len = title.shape[0]
    
    #tensor to store decoder outputs
    outputs = torch.zeros(max_len, batch_size, self.title_vocab_size).to(DEVICE)   
    #first input to the decoder is the &lt;sos&gt; tokens
    output = title[0,:]

    if self.show_attention:
        attentions = torch.zeros((max_len, batch_size, encoder_outputs.shape[0])).to(DEVICE)
        logger.debug(f&#34;Attentions viz shape: {attentions.shape}&#34;)
    
    hidden= self.decoder.init_hidden(batch_size)
    
    for t in range(1, max_len):
        if self.show_attention:
            output, hidden, attention = self.decoder(output, hidden, encoder_outputs)
            logger.debug(f&#34;Attentions output shape: {attention.shape}&#34;)
            attentions[t] = attention
        else:
            output, hidden = self.decoder(output, hidden, encoder_outputs)
        outputs[t] = output
        teacher_force = random.random() &lt; teacher_forcing_ratio
        top1 = output.max(1)[1]
        output = (title[t] if teacher_force else top1)

    logger.debug(f&#34;Model output shape: {outputs.shape}&#34;)

    if self.show_attention:
        return outputs, attentions
    
    return outputs</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ncn.model.TDNN"><code class="flex name class">
<span>class <span class="ident">TDNN</span></span>
<span>(</span><span>filter_size: int, embed_size: int, num_filters: int)</span>
</code></dt>
<dd>
<section class="desc"><p>Single TDNN Block for the neural citation network.
Implementation is based on:<br>
<a href="https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf.">https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf.</a><br>
Consists of the following layers (in order): Convolution, ReLu, Batchnorm, MaxPool.
</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li><strong>filter_size</strong> <em>(int)</em>: filter length for the convolutional operation
</li>
<li><strong>embed_size</strong> <em>(int)</em>: Dimension of the input word embeddings
</li>
<li><strong>num_filters</strong> <em>(int)</em>: Number of convolutional filters</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class TDNN(nn.Module):
    &#34;&#34;&#34;
    Single TDNN Block for the neural citation network.
    Implementation is based on:  
    https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf.  
    Consists of the following layers (in order): Convolution, ReLu, Batchnorm, MaxPool.  

    ## Parameters:   

    - **filter_size** *(int)*: filter length for the convolutional operation  
    - **embed_size** *(int)*: Dimension of the input word embeddings  
    - **num_filters** *(int)*: Number of convolutional filters  
    &#34;&#34;&#34;

    def __init__(self, filter_size: int, 
                       embed_size: int, 
                       num_filters: int):
        super().__init__()
        # model input shape: [N: batch size, D: embedding dimensions, L: sequence length]
        # no bias to avoid accumulating biases on padding
        self.conv = nn.Conv2d(1, num_filters, kernel_size=(embed_size, filter_size), bias=False)

    def forward(self, x: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;
        ## Input:  

        - **Embedded sequence** *(batch size, seq length, embedding dimensions)*:  
            Tensor containing a batch of embedded input sequences.

        ## Output:  

        - **Convolved sequence** *(batch_size, num_filters)*:  
            Tensor containing the output. 
        &#34;&#34;&#34;
        # [N: batch size, L: seq length, D embedding dimensions] -&gt; [N: batch size, D embedding dimensions, L: seq length]
        x = x.permute(0, 2, 1)
        # output shape: [N: batch size, 1: channels, D: embedding dimensions, L: sequence length]
        x = x.unsqueeze(1)


        # output shape: batch_size, num_filters, 1, f(seq length)
        x = F.relu(self.conv(x))
        pool_size = x.shape[-1]

        # output shape: batch_size, num_filters, 1, 1
        x = F.max_pool2d(x, kernel_size=pool_size)

        # output shape: batch_size, 1, num_filters, 1
        return x.permute(0, 2, 1, 3)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ncn.model.TDNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) -> torch.Tensor</span>
</code></dt>
<dd>
<section class="desc"><h2 id="input">Input:</h2>
<ul>
<li><strong>Embedded sequence</strong> <em>(batch size, seq length, embedding dimensions)</em>:<br>
Tensor containing a batch of embedded input sequences.</li>
</ul>
<h2 id="output">Output:</h2>
<ul>
<li><strong>Convolved sequence</strong> <em>(batch_size, num_filters)</em>:<br>
Tensor containing the output.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, x: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    ## Input:  

    - **Embedded sequence** *(batch size, seq length, embedding dimensions)*:  
        Tensor containing a batch of embedded input sequences.

    ## Output:  

    - **Convolved sequence** *(batch_size, num_filters)*:  
        Tensor containing the output. 
    &#34;&#34;&#34;
    # [N: batch size, L: seq length, D embedding dimensions] -&gt; [N: batch size, D embedding dimensions, L: seq length]
    x = x.permute(0, 2, 1)
    # output shape: [N: batch size, 1: channels, D: embedding dimensions, L: sequence length]
    x = x.unsqueeze(1)


    # output shape: batch_size, num_filters, 1, f(seq length)
    x = F.relu(self.conv(x))
    pool_size = x.shape[-1]

    # output shape: batch_size, num_filters, 1, 1
    x = F.max_pool2d(x, kernel_size=pool_size)

    # output shape: batch_size, 1, num_filters, 1
    return x.permute(0, 2, 1, 3)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ncn.model.TDNNEncoder"><code class="flex name class">
<span>class <span class="ident">TDNNEncoder</span></span>
<span>(</span><span>filters: List[int], num_filters: int, embed_size: int)</span>
</code></dt>
<dd>
<section class="desc"><p>Encoder Module based on the TDNN architecture.
Applies as list of filters with different region sizes on an input sequence.<br>
The resulting feature maps are then allowed to interact with each other across a fully connected layer.
</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li><strong>filters</strong> <em>(Filters)</em>: List of integers determining the filter lengths.
</li>
<li><strong>num_filters</strong> <em>(int)</em>: Number of filters applied in the TDNN convolutional layers.
</li>
<li><strong>embed_size</strong> <em>(int)</em>: Dimensions of the used embeddings.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class TDNNEncoder(nn.Module):
    &#34;&#34;&#34;
    Encoder Module based on the TDNN architecture.
    Applies as list of filters with different region sizes on an input sequence.  
    The resulting feature maps are then allowed to interact with each other across a fully connected layer.  
    
    ## Parameters:  
    
    - **filters** *(Filters)*: List of integers determining the filter lengths.    
    - **num_filters** *(int)*: Number of filters applied in the TDNN convolutional layers.  
    - **embed_size** *(int)*: Dimensions of the used embeddings.  
    &#34;&#34;&#34;
    def __init__(self, filters: Filters,
                       num_filters: int,
                       embed_size: int):

        super().__init__()
        self.filter_list = filters
        self.num_filters = num_filters
        self._num_filters_total = len(filters)*num_filters

        self.encoder = nn.ModuleList([TDNN(filter_size=f, embed_size = embed_size, num_filters=num_filters).to(DEVICE) 
                                        for f in self.filter_list])
        self.fc = nn.Linear(self._num_filters_total, self._num_filters_total)


    def forward(self, x: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;
        ## Input:  

        - **Embeddings** *(batch size, seq length, embedding dimensions)*:
            Embedded input sequence.  

        ## Output:  

        - **Encodings** *(number of filter sizes, batch size, # filters)*:
            Tensor containing the complete context/author encodings.
        &#34;&#34;&#34;
        x = [encoder(x) for encoder in self.encoder]
        assert len(set([e.shape[0] for e in x])) == 1, &#34;Batch sizes don&#39;t match!&#34;


        # output shape: batch_size, list_length, num_filters
        x = torch.cat(x, dim=1).squeeze(3)

        batch_size = x.shape[0]

        # output shape: batch_size, list_length*num_filters
        x = x.view(batch_size, -1)

        # apply nonlinear mapping
        x = torch.tanh(self.fc(x))

        # output shape: list_length, batch_size, num_filters
        return x.view(len(self.filter_list), -1, self.num_filters)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ncn.model.TDNNEncoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) -> torch.Tensor</span>
</code></dt>
<dd>
<section class="desc"><h2 id="input">Input:</h2>
<ul>
<li><strong>Embeddings</strong> <em>(batch size, seq length, embedding dimensions)</em>:
Embedded input sequence.
</li>
</ul>
<h2 id="output">Output:</h2>
<ul>
<li><strong>Encodings</strong> <em>(number of filter sizes, batch size, # filters)</em>:
Tensor containing the complete context/author encodings.</li>
</ul></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, x: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    ## Input:  

    - **Embeddings** *(batch size, seq length, embedding dimensions)*:
        Embedded input sequence.  

    ## Output:  

    - **Encodings** *(number of filter sizes, batch size, # filters)*:
        Tensor containing the complete context/author encodings.
    &#34;&#34;&#34;
    x = [encoder(x) for encoder in self.encoder]
    assert len(set([e.shape[0] for e in x])) == 1, &#34;Batch sizes don&#39;t match!&#34;


    # output shape: batch_size, list_length, num_filters
    x = torch.cat(x, dim=1).squeeze(3)

    batch_size = x.shape[0]

    # output shape: batch_size, list_length*num_filters
    x = x.view(batch_size, -1)

    # apply nonlinear mapping
    x = torch.tanh(self.fc(x))

    # output shape: list_length, batch_size, num_filters
    return x.view(len(self.filter_list), -1, self.num_filters)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ncn" href="index.html">ncn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ncn.model.Attention" href="#ncn.model.Attention">Attention</a></code></h4>
<ul class="">
<li><code><a title="ncn.model.Attention.forward" href="#ncn.model.Attention.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ncn.model.Decoder" href="#ncn.model.Decoder">Decoder</a></code></h4>
<ul class="">
<li><code><a title="ncn.model.Decoder.forward" href="#ncn.model.Decoder.forward">forward</a></code></li>
<li><code><a title="ncn.model.Decoder.init_hidden" href="#ncn.model.Decoder.init_hidden">init_hidden</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ncn.model.NCNEncoder" href="#ncn.model.NCNEncoder">NCNEncoder</a></code></h4>
<ul class="">
<li><code><a title="ncn.model.NCNEncoder.forward" href="#ncn.model.NCNEncoder.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ncn.model.NeuralCitationNetwork" href="#ncn.model.NeuralCitationNetwork">NeuralCitationNetwork</a></code></h4>
<ul class="">
<li><code><a title="ncn.model.NeuralCitationNetwork.count_parameters" href="#ncn.model.NeuralCitationNetwork.count_parameters">count_parameters</a></code></li>
<li><code><a title="ncn.model.NeuralCitationNetwork.forward" href="#ncn.model.NeuralCitationNetwork.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ncn.model.TDNN" href="#ncn.model.TDNN">TDNN</a></code></h4>
<ul class="">
<li><code><a title="ncn.model.TDNN.forward" href="#ncn.model.TDNN.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ncn.model.TDNNEncoder" href="#ncn.model.TDNNEncoder">TDNNEncoder</a></code></h4>
<ul class="">
<li><code><a title="ncn.model.TDNNEncoder.forward" href="#ncn.model.TDNNEncoder.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>